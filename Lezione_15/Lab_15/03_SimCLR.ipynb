{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51056cad-25b1-4baa-b011-93c0b1885879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2618\n",
      "Loss: 3.2524\n",
      "Loss: 3.0144\n",
      "Loss: 3.1063\n",
      "Loss: 2.9531\n",
      "Loss: 2.9929\n",
      "Loss: 2.7704\n",
      "Loss: 2.9567\n",
      "Loss: 2.8871\n",
      "Loss: 2.7814\n",
      "Loss: 2.6348\n",
      "Loss: 2.7831\n",
      "Loss: 2.6347\n",
      "Loss: 2.5610\n",
      "Loss: 2.3411\n",
      "Loss: 3.1918\n",
      "Loss: 2.6296\n",
      "Loss: 2.8086\n",
      "Loss: 2.7367\n",
      "Loss: 2.5065\n",
      "Loss: 2.6690\n",
      "Loss: 2.6929\n",
      "Loss: 2.9132\n",
      "Loss: 2.3690\n",
      "Loss: 2.4448\n",
      "Loss: 2.6789\n",
      "Loss: 2.5385\n",
      "Loss: 2.4750\n",
      "Loss: 2.5035\n",
      "Loss: 2.4985\n",
      "Loss: 2.7757\n",
      "Loss: 2.5425\n",
      "Loss: 2.5418\n",
      "Loss: 2.5142\n",
      "Loss: 2.5408\n",
      "Loss: 2.6757\n",
      "Loss: 2.2696\n",
      "Loss: 2.2776\n",
      "Loss: 2.5645\n",
      "Loss: 2.7233\n",
      "Loss: 2.5119\n",
      "Loss: 2.4778\n",
      "Loss: 2.6486\n",
      "Loss: 2.3645\n",
      "Loss: 2.5653\n",
      "Loss: 2.3759\n",
      "Loss: 2.3978\n",
      "Loss: 2.3867\n",
      "Loss: 2.6243\n",
      "Loss: 2.3129\n",
      "Loss: 2.2547\n",
      "Loss: 2.7386\n",
      "Loss: 2.3332\n",
      "Loss: 2.5635\n",
      "Loss: 2.4412\n",
      "Loss: 2.7544\n",
      "Loss: 2.4257\n",
      "Loss: 2.3533\n",
      "Loss: 2.5612\n",
      "Loss: 2.4589\n",
      "Loss: 2.2187\n",
      "Loss: 2.3256\n",
      "Loss: 2.3865\n",
      "Loss: 2.2964\n",
      "Loss: 2.6938\n",
      "Loss: 2.3360\n",
      "Loss: 2.4444\n",
      "Loss: 2.5634\n",
      "Loss: 2.5381\n",
      "Loss: 2.2305\n",
      "Loss: 2.3927\n",
      "Loss: 2.3927\n",
      "Loss: 2.6843\n",
      "Loss: 2.7041\n",
      "Loss: 2.5965\n",
      "Loss: 2.7713\n",
      "Loss: 2.3335\n",
      "Loss: 2.3773\n",
      "Loss: 2.3663\n",
      "Loss: 2.2062\n",
      "Loss: 2.3562\n",
      "Loss: 2.3563\n",
      "Loss: 2.3570\n",
      "Loss: 2.3200\n",
      "Loss: 2.5255\n",
      "Loss: 2.6042\n",
      "Loss: 2.3278\n",
      "Loss: 2.3909\n",
      "Loss: 2.2297\n",
      "Loss: 2.1641\n",
      "Loss: 2.3800\n",
      "Loss: 2.4941\n",
      "Loss: 2.3362\n",
      "Loss: 2.2425\n",
      "Loss: 2.3490\n",
      "Loss: 2.2886\n",
      "Loss: 2.4264\n",
      "Loss: 2.4238\n",
      "Loss: 2.3483\n",
      "Loss: 2.5556\n",
      "Loss: 2.3947\n",
      "Loss: 2.4893\n",
      "Loss: 2.4115\n",
      "Loss: 2.2991\n",
      "Loss: 2.4903\n",
      "Loss: 2.3550\n",
      "Loss: 2.2644\n",
      "Loss: 2.4998\n",
      "Loss: 2.4874\n",
      "Loss: 2.3527\n",
      "Loss: 2.3238\n",
      "Loss: 2.4851\n",
      "Loss: 2.5090\n",
      "Loss: 2.5824\n",
      "Loss: 2.2620\n",
      "Loss: 2.2865\n",
      "Loss: 2.2618\n",
      "Loss: 2.2608\n",
      "Loss: 2.5016\n",
      "Loss: 2.4064\n",
      "Loss: 2.3927\n",
      "Loss: 2.2547\n",
      "Loss: 2.2118\n",
      "Loss: 2.3178\n",
      "Loss: 2.4065\n",
      "Loss: 2.3444\n",
      "Loss: 2.6240\n",
      "Loss: 2.6723\n",
      "Loss: 2.4712\n",
      "Loss: 2.2722\n",
      "Loss: 2.5714\n",
      "Loss: 2.4698\n",
      "Loss: 2.3351\n",
      "Loss: 2.2908\n",
      "Loss: 2.3409\n",
      "Loss: 2.3885\n",
      "Loss: 2.3165\n",
      "Loss: 2.1990\n",
      "Loss: 2.2218\n",
      "Loss: 2.3843\n",
      "Loss: 2.2849\n",
      "Loss: 2.2596\n",
      "Loss: 2.5001\n",
      "Loss: 2.5282\n",
      "Loss: 2.3692\n",
      "Loss: 2.3210\n",
      "Loss: 2.6150\n",
      "Loss: 2.2584\n",
      "Loss: 2.2898\n",
      "Loss: 2.3493\n",
      "Loss: 2.5754\n",
      "Loss: 2.0977\n",
      "Loss: 2.3935\n",
      "Loss: 2.0529\n",
      "Loss: 2.3515\n",
      "Loss: 2.1299\n",
      "Loss: 2.3296\n",
      "Loss: 2.3336\n",
      "Loss: 2.3584\n",
      "Loss: 2.1886\n",
      "Loss: 2.4937\n",
      "Loss: 2.2251\n",
      "Loss: 2.3682\n",
      "Loss: 2.3104\n",
      "Loss: 2.2909\n",
      "Loss: 2.2781\n",
      "Loss: 2.2924\n",
      "Loss: 2.6023\n",
      "Loss: 2.5317\n",
      "Loss: 2.2921\n",
      "Loss: 2.3990\n",
      "Loss: 2.2778\n",
      "Loss: 2.2315\n",
      "Loss: 2.2497\n",
      "Loss: 2.3038\n",
      "Loss: 2.1445\n",
      "Loss: 2.4027\n",
      "Loss: 2.4448\n",
      "Loss: 2.2549\n",
      "Loss: 2.7107\n",
      "Loss: 2.2007\n",
      "Loss: 2.2079\n",
      "Loss: 2.2860\n",
      "Loss: 2.2505\n",
      "Loss: 2.2767\n",
      "Loss: 2.6512\n",
      "Loss: 1.9882\n",
      "Loss: 2.3210\n",
      "Loss: 2.3942\n",
      "Loss: 2.5404\n",
      "Loss: 2.6940\n",
      "Loss: 2.6236\n",
      "Loss: 2.3241\n",
      "Loss: 2.4290\n",
      "Loss: 2.3498\n",
      "Loss: 2.1694\n",
      "Loss: 2.4959\n",
      "Loss: 2.0867\n",
      "Loss: 2.5239\n",
      "Loss: 2.3430\n",
      "Loss: 2.4440\n",
      "Loss: 2.2274\n",
      "Loss: 2.3371\n",
      "Loss: 2.2674\n",
      "Loss: 2.2520\n",
      "Loss: 2.6101\n",
      "Loss: 2.1112\n",
      "Loss: 2.4946\n",
      "Loss: 2.2953\n",
      "Loss: 2.4172\n",
      "Loss: 2.1885\n",
      "Loss: 2.3346\n",
      "Loss: 2.5537\n",
      "Loss: 2.2103\n",
      "Loss: 2.4359\n",
      "Loss: 2.2037\n",
      "Loss: 2.1895\n",
      "Loss: 2.2176\n",
      "Loss: 2.2197\n",
      "Loss: 2.3007\n",
      "Loss: 2.4293\n",
      "Loss: 2.3257\n",
      "Loss: 2.2521\n",
      "Loss: 2.5600\n",
      "Loss: 2.3753\n",
      "Loss: 2.5509\n",
      "Loss: 2.3782\n",
      "Loss: 2.4296\n",
      "Loss: 2.2751\n",
      "Loss: 2.2828\n",
      "Loss: 2.3082\n",
      "Loss: 2.3437\n",
      "Loss: 2.3451\n",
      "Loss: 2.2449\n",
      "Loss: 2.1900\n",
      "Loss: 2.2730\n",
      "Loss: 2.4822\n",
      "Loss: 2.4933\n",
      "Loss: 2.7087\n",
      "Loss: 2.2947\n",
      "Loss: 2.1997\n",
      "Loss: 2.0172\n",
      "Loss: 2.2797\n",
      "Loss: 2.0101\n",
      "Loss: 2.1787\n",
      "Loss: 2.7201\n",
      "Loss: 2.2397\n",
      "Loss: 2.4321\n",
      "Loss: 2.1222\n",
      "Loss: 2.4590\n",
      "Loss: 2.2252\n",
      "Loss: 2.3827\n",
      "Loss: 2.2513\n",
      "Loss: 2.4567\n",
      "Loss: 2.2459\n",
      "Loss: 2.1629\n",
      "Loss: 2.6540\n",
      "Loss: 2.4284\n",
      "Loss: 2.5694\n",
      "Loss: 2.0123\n",
      "Loss: 2.0386\n",
      "Loss: 2.3289\n",
      "Loss: 2.2346\n",
      "Loss: 2.1709\n",
      "Loss: 2.1935\n",
      "Loss: 2.4977\n",
      "Loss: 2.1482\n",
      "Loss: 2.0538\n",
      "Loss: 2.3552\n",
      "Loss: 2.2283\n",
      "Loss: 2.2698\n",
      "Loss: 2.1920\n",
      "Loss: 2.2585\n",
      "Loss: 2.2570\n",
      "Loss: 2.4215\n",
      "Loss: 2.0814\n",
      "Loss: 2.0973\n",
      "Loss: 2.2443\n",
      "Loss: 2.3555\n",
      "Loss: 2.1786\n",
      "Loss: 2.2142\n",
      "Loss: 2.3948\n",
      "Loss: 2.3244\n",
      "Loss: 2.2406\n",
      "Loss: 2.1148\n",
      "Loss: 2.3919\n",
      "Loss: 2.0845\n",
      "Loss: 2.2631\n",
      "Loss: 2.0465\n",
      "Loss: 2.2510\n",
      "Loss: 2.2603\n",
      "Loss: 2.1339\n",
      "Loss: 2.2504\n",
      "Loss: 2.3297\n",
      "Loss: 2.4381\n",
      "Loss: 2.2852\n",
      "Loss: 2.4985\n",
      "Loss: 2.4897\n",
      "Loss: 2.3459\n",
      "Loss: 2.3462\n",
      "Loss: 2.4644\n",
      "Loss: 2.4389\n",
      "Loss: 2.1535\n",
      "Loss: 2.4217\n",
      "Loss: 2.1703\n",
      "Loss: 2.2595\n",
      "Loss: 2.2296\n",
      "Loss: 2.2369\n",
      "Loss: 2.2719\n",
      "Loss: 2.1192\n",
      "Loss: 2.2980\n",
      "Loss: 2.3499\n",
      "Loss: 2.4111\n",
      "Loss: 2.3196\n",
      "Loss: 2.6603\n",
      "Loss: 2.0573\n",
      "Loss: 2.2256\n",
      "Loss: 2.2233\n",
      "Loss: 2.3151\n",
      "Loss: 2.4896\n",
      "Loss: 2.0786\n",
      "Loss: 2.2261\n",
      "Loss: 2.4280\n",
      "Loss: 2.2202\n",
      "Loss: 2.2452\n",
      "Loss: 2.3941\n",
      "Loss: 2.4789\n",
      "Loss: 2.3052\n",
      "Loss: 2.3313\n",
      "Loss: 2.3146\n",
      "Loss: 2.2480\n",
      "Loss: 2.2320\n",
      "Loss: 2.1904\n",
      "Loss: 2.1717\n",
      "Loss: 2.2050\n",
      "Loss: 2.3474\n",
      "Loss: 2.1541\n",
      "Loss: 2.3711\n",
      "Loss: 2.1909\n",
      "Loss: 2.0742\n",
      "Loss: 2.2537\n",
      "Loss: 2.1164\n",
      "Loss: 2.0070\n",
      "Loss: 2.3086\n",
      "Loss: 2.2048\n",
      "Loss: 2.1485\n",
      "Loss: 2.1034\n",
      "Loss: 2.5794\n",
      "Loss: 2.3415\n",
      "Loss: 2.2694\n",
      "Loss: 2.2335\n",
      "Loss: 2.5824\n",
      "Loss: 2.4638\n",
      "Loss: 2.5350\n",
      "Loss: 2.2968\n",
      "Loss: 2.0833\n",
      "Loss: 2.4231\n",
      "Loss: 2.3786\n",
      "Loss: 2.3449\n",
      "Loss: 2.1575\n",
      "Loss: 2.1223\n",
      "Loss: 2.2681\n",
      "Loss: 2.3216\n",
      "Loss: 2.0636\n",
      "Loss: 2.4046\n",
      "Loss: 2.1907\n",
      "Loss: 2.2305\n",
      "Loss: 2.4146\n",
      "Loss: 2.2034\n",
      "Loss: 2.2725\n",
      "Loss: 2.1707\n",
      "Loss: 2.6015\n",
      "Loss: 2.3951\n",
      "Loss: 2.2386\n",
      "Loss: 2.2499\n",
      "Loss: 2.3480\n",
      "Loss: 2.0326\n",
      "Loss: 2.3170\n",
      "Loss: 2.4111\n",
      "Loss: 2.1926\n",
      "Loss: 2.3710\n",
      "Loss: 2.4493\n",
      "Loss: 2.4281\n",
      "Loss: 2.0909\n",
      "Loss: 1.9739\n",
      "Loss: 2.0787\n",
      "Loss: 2.0919\n",
      "Loss: 2.0321\n",
      "Loss: 2.2379\n",
      "Loss: 2.2487\n",
      "Loss: 2.3733\n",
      "Loss: 2.3380\n",
      "Loss: 2.2592\n",
      "Loss: 2.1699\n",
      "Loss: 2.2111\n",
      "Loss: 2.0878\n",
      "Loss: 2.4597\n",
      "Loss: 2.1284\n",
      "Loss: 2.0651\n",
      "Loss: 2.2520\n",
      "Loss: 2.3467\n",
      "Loss: 2.3138\n",
      "Loss: 2.2313\n",
      "Loss: 2.2248\n",
      "Loss: 2.0055\n",
      "Loss: 2.3734\n",
      "Loss: 2.4785\n",
      "Loss: 2.4762\n",
      "Loss: 2.3384\n",
      "Loss: 2.0574\n",
      "Loss: 2.2883\n",
      "Loss: 2.1392\n",
      "Loss: 2.1591\n",
      "Loss: 2.0842\n",
      "Loss: 2.4794\n",
      "Loss: 2.5580\n",
      "Loss: 2.0704\n",
      "Loss: 2.2565\n",
      "Loss: 2.3970\n",
      "Loss: 2.0724\n",
      "Loss: 2.2897\n",
      "Loss: 2.1760\n",
      "Loss: 2.0725\n",
      "Loss: 2.2489\n",
      "Loss: 2.2076\n",
      "Loss: 2.2997\n",
      "Loss: 2.3897\n",
      "Loss: 2.2443\n",
      "Loss: 2.2456\n",
      "Loss: 2.0868\n",
      "Loss: 2.5462\n",
      "Loss: 2.0387\n",
      "Loss: 2.0750\n",
      "Loss: 2.7196\n",
      "Loss: 2.2619\n",
      "Loss: 2.4909\n",
      "Loss: 2.3087\n",
      "Loss: 2.3356\n",
      "Loss: 2.2530\n",
      "Loss: 2.4808\n",
      "Loss: 2.4466\n",
      "Loss: 2.4261\n",
      "Loss: 2.1726\n",
      "Loss: 2.1068\n",
      "Loss: 2.1938\n",
      "Loss: 2.2899\n",
      "Loss: 1.9858\n",
      "Loss: 2.2541\n",
      "Loss: 2.4127\n",
      "Loss: 2.6076\n",
      "Loss: 2.4578\n",
      "Loss: 2.2773\n",
      "Loss: 2.1312\n",
      "Loss: 2.4172\n",
      "Loss: 2.1918\n",
      "Loss: 2.4892\n",
      "Loss: 2.1751\n",
      "Loss: 2.1859\n",
      "Loss: 2.1422\n",
      "Loss: 2.2429\n",
      "Loss: 2.2529\n",
      "Loss: 2.3867\n",
      "Loss: 2.2231\n",
      "Loss: 2.2558\n",
      "Loss: 2.3986\n",
      "Loss: 2.2279\n",
      "Loss: 2.1071\n",
      "Loss: 2.5269\n",
      "Loss: 2.2337\n",
      "Loss: 2.3181\n",
      "Loss: 2.2177\n",
      "Loss: 2.4365\n",
      "Loss: 2.2557\n",
      "Loss: 2.3354\n",
      "Loss: 2.3207\n",
      "Loss: 2.2760\n",
      "Loss: 2.1472\n",
      "Loss: 2.1360\n",
      "Loss: 2.3596\n",
      "Loss: 2.3889\n",
      "Loss: 2.0419\n",
      "Loss: 2.3006\n",
      "Loss: 2.0757\n",
      "Loss: 2.1927\n",
      "Loss: 2.3704\n",
      "Loss: 2.0276\n",
      "Loss: 2.5482\n",
      "Loss: 2.3315\n",
      "Loss: 2.1774\n",
      "Loss: 2.2450\n",
      "Loss: 2.0695\n",
      "Loss: 2.3073\n",
      "Loss: 2.2681\n",
      "Loss: 2.0854\n",
      "Loss: 2.2817\n",
      "Loss: 2.3230\n",
      "Loss: 2.2366\n",
      "Loss: 2.2346\n",
      "Loss: 2.1804\n",
      "Loss: 2.0813\n",
      "Loss: 2.0929\n",
      "Loss: 2.2204\n",
      "Loss: 2.4240\n",
      "Loss: 1.9881\n",
      "Loss: 2.1197\n",
      "Loss: 2.4249\n",
      "Loss: 2.3649\n",
      "Loss: 2.2690\n",
      "Loss: 2.1216\n",
      "Loss: 2.3612\n",
      "Loss: 2.1785\n",
      "Loss: 2.1000\n",
      "Loss: 2.3173\n",
      "Loss: 2.3654\n",
      "Loss: 2.1654\n",
      "Loss: 2.1080\n",
      "Loss: 2.5790\n",
      "Loss: 2.4762\n",
      "Loss: 2.2614\n",
      "Loss: 2.1563\n",
      "Loss: 2.2814\n",
      "Loss: 2.4774\n",
      "Loss: 2.3286\n",
      "Loss: 2.0245\n",
      "Loss: 2.2191\n",
      "Loss: 2.5497\n",
      "Loss: 2.3048\n",
      "Loss: 2.1496\n",
      "Loss: 2.3770\n",
      "Loss: 2.5558\n",
      "Loss: 2.4453\n",
      "Loss: 2.1616\n",
      "Loss: 2.2673\n",
      "Loss: 2.3209\n",
      "Loss: 2.2587\n",
      "Loss: 2.0464\n",
      "Loss: 2.0768\n",
      "Loss: 2.1379\n",
      "Loss: 2.3613\n",
      "Loss: 2.2638\n",
      "Loss: 2.0932\n",
      "Loss: 2.4039\n",
      "Loss: 2.3892\n",
      "Loss: 2.0799\n",
      "Loss: 2.1000\n",
      "Loss: 2.4175\n",
      "Loss: 2.2145\n",
      "Loss: 2.0959\n",
      "Loss: 2.1617\n",
      "Loss: 2.2737\n",
      "Loss: 2.2839\n",
      "Loss: 2.1611\n",
      "Loss: 2.3859\n",
      "Loss: 2.3216\n",
      "Loss: 2.1237\n",
      "Loss: 2.1952\n",
      "Loss: 2.3344\n",
      "Loss: 2.2559\n",
      "Loss: 2.1625\n",
      "Loss: 1.9509\n",
      "Loss: 2.1805\n",
      "Loss: 2.0904\n",
      "Loss: 2.2851\n",
      "Loss: 2.3504\n",
      "Loss: 2.2710\n",
      "Loss: 2.3311\n",
      "Loss: 2.2819\n",
      "Loss: 2.3171\n",
      "Loss: 2.4403\n",
      "Loss: 2.1400\n",
      "Loss: 2.2684\n",
      "Loss: 2.1080\n",
      "Loss: 2.2359\n",
      "Loss: 2.2517\n",
      "Loss: 2.4372\n",
      "Loss: 2.1044\n",
      "Loss: 2.2645\n",
      "Loss: 2.1302\n",
      "Loss: 2.2381\n",
      "Loss: 2.2476\n",
      "Loss: 2.0170\n",
      "Loss: 2.0878\n",
      "Loss: 2.0480\n",
      "Loss: 2.2552\n",
      "Loss: 2.3787\n",
      "Loss: 2.2186\n",
      "Loss: 2.0921\n",
      "Loss: 2.2625\n",
      "Loss: 2.0926\n",
      "Loss: 2.4861\n",
      "Loss: 2.3039\n",
      "Loss: 2.1619\n",
      "Loss: 2.4994\n",
      "Loss: 2.0939\n",
      "Loss: 2.3938\n",
      "Loss: 2.3600\n",
      "Loss: 2.2161\n",
      "Loss: 2.4243\n",
      "Loss: 2.4659\n",
      "Loss: 2.2424\n",
      "Loss: 2.1175\n",
      "Loss: 2.3411\n",
      "Loss: 2.3181\n",
      "Loss: 2.3818\n",
      "Loss: 2.2268\n",
      "Loss: 2.2628\n",
      "Loss: 2.1541\n",
      "Loss: 2.2316\n",
      "Loss: 2.2474\n",
      "Loss: 2.3615\n",
      "Loss: 2.3117\n",
      "Loss: 2.0136\n",
      "Loss: 2.2067\n",
      "Loss: 2.1752\n",
      "Loss: 2.1753\n",
      "Loss: 2.3265\n",
      "Loss: 2.0865\n",
      "Loss: 2.2795\n",
      "Loss: 2.1630\n",
      "Loss: 2.2021\n",
      "Loss: 2.2220\n",
      "Loss: 2.1456\n",
      "Loss: 2.3946\n",
      "Loss: 2.0561\n",
      "Loss: 2.2890\n",
      "Loss: 2.4022\n",
      "Loss: 2.2015\n",
      "Loss: 2.2887\n",
      "Loss: 2.2497\n",
      "Loss: 2.2838\n",
      "Loss: 2.0705\n",
      "Loss: 2.1019\n",
      "Loss: 2.2519\n",
      "Loss: 1.8402\n",
      "Loss: 2.1852\n",
      "Loss: 2.2452\n",
      "Loss: 2.2757\n",
      "Loss: 2.2601\n",
      "Loss: 2.3466\n",
      "Loss: 2.4078\n",
      "Loss: 2.0989\n",
      "Loss: 2.3912\n",
      "Loss: 2.0811\n",
      "Loss: 2.1501\n",
      "Loss: 2.0656\n",
      "Loss: 2.4398\n",
      "Loss: 2.2085\n",
      "Loss: 2.2857\n",
      "Loss: 2.4845\n",
      "Loss: 2.5203\n",
      "Loss: 2.1938\n",
      "Loss: 2.2377\n",
      "Loss: 2.3851\n",
      "Loss: 2.0330\n",
      "Loss: 2.1579\n",
      "Loss: 2.3278\n",
      "Loss: 2.2106\n",
      "Loss: 2.1811\n",
      "Loss: 2.4944\n",
      "Loss: 2.1728\n",
      "Loss: 2.0749\n",
      "Loss: 2.2386\n",
      "Loss: 2.4979\n",
      "Loss: 2.3385\n",
      "Loss: 2.0785\n",
      "Loss: 2.0598\n",
      "Loss: 2.0943\n",
      "Loss: 2.1970\n",
      "Loss: 2.1614\n",
      "Loss: 2.1217\n",
      "Loss: 2.1355\n",
      "Loss: 2.1690\n",
      "Loss: 2.2155\n",
      "Loss: 2.3529\n",
      "Loss: 2.1411\n",
      "Loss: 2.0647\n",
      "Loss: 2.2838\n",
      "Loss: 2.1542\n",
      "Loss: 2.1636\n",
      "Loss: 2.2060\n",
      "Loss: 2.1322\n",
      "Loss: 1.9735\n",
      "Loss: 2.0646\n",
      "Loss: 2.2592\n",
      "Loss: 2.0813\n",
      "Loss: 1.9971\n",
      "Loss: 2.2499\n",
      "Loss: 2.0819\n",
      "Loss: 2.2320\n",
      "Loss: 2.2985\n",
      "Loss: 2.4894\n",
      "Loss: 1.9965\n",
      "Loss: 2.1213\n",
      "Loss: 2.2738\n",
      "Loss: 2.2231\n",
      "Loss: 2.0500\n",
      "Loss: 2.2669\n",
      "Loss: 2.4226\n",
      "Loss: 2.1909\n",
      "Loss: 1.9232\n",
      "Loss: 2.3172\n",
      "Loss: 2.0858\n",
      "Loss: 2.1544\n",
      "Loss: 2.6115\n",
      "Loss: 2.3685\n",
      "Loss: 2.1619\n",
      "Loss: 2.4429\n",
      "Loss: 2.0539\n",
      "Loss: 2.4551\n",
      "Loss: 2.0898\n",
      "Loss: 2.2016\n",
      "Loss: 2.0964\n",
      "Loss: 2.4357\n",
      "Loss: 2.2510\n",
      "Loss: 2.2788\n",
      "Loss: 2.1943\n",
      "Loss: 2.1521\n",
      "Loss: 2.0173\n",
      "Loss: 2.0728\n",
      "Loss: 2.0641\n",
      "Loss: 2.2260\n",
      "Loss: 2.2729\n",
      "Loss: 2.0721\n",
      "Loss: 2.1861\n",
      "Loss: 2.1053\n",
      "Loss: 2.4696\n",
      "Loss: 2.2518\n",
      "Loss: 2.3289\n",
      "Loss: 2.4852\n",
      "Loss: 2.0998\n",
      "Loss: 2.5626\n",
      "Loss: 1.9928\n",
      "Loss: 2.0658\n",
      "Loss: 2.2364\n",
      "Loss: 2.3813\n",
      "Loss: 2.2707\n",
      "Loss: 2.1763\n",
      "Loss: 1.9404\n",
      "Loss: 2.2259\n",
      "Loss: 2.2722\n",
      "Loss: 2.1288\n",
      "Loss: 2.2463\n",
      "Loss: 2.4145\n",
      "Loss: 2.1116\n",
      "Loss: 2.0578\n",
      "Loss: 2.2815\n",
      "Loss: 2.1729\n",
      "Loss: 2.0788\n",
      "Loss: 2.0856\n",
      "Loss: 2.1353\n",
      "Loss: 2.3923\n",
      "Loss: 2.2279\n",
      "Loss: 2.2902\n",
      "Loss: 2.0549\n",
      "Loss: 2.3649\n",
      "Loss: 2.2834\n",
      "Loss: 2.3747\n",
      "Loss: 2.3174\n",
      "Loss: 2.2480\n",
      "Loss: 2.5467\n",
      "Loss: 2.2082\n",
      "Loss: 2.0266\n",
      "Loss: 2.2560\n",
      "Loss: 2.3140\n",
      "Loss: 2.2827\n",
      "Loss: 2.2837\n",
      "Loss: 2.1383\n",
      "Loss: 2.0238\n",
      "Loss: 2.0153\n",
      "Loss: 2.0593\n",
      "Loss: 1.9998\n",
      "Loss: 2.1140\n",
      "Loss: 2.2979\n",
      "Loss: 2.3378\n",
      "Loss: 2.2671\n",
      "Loss: 2.5048\n",
      "Loss: 2.1700\n",
      "Loss: 2.1972\n",
      "Loss: 2.3634\n",
      "Loss: 2.1691\n",
      "Loss: 2.2332\n",
      "Loss: 2.2458\n",
      "Loss: 2.3125\n",
      "Loss: 2.2435\n",
      "Loss: 2.2445\n",
      "Loss: 1.9831\n",
      "Loss: 2.1612\n",
      "Loss: 2.1341\n",
      "Loss: 2.2438\n",
      "Loss: 2.0533\n",
      "Loss: 2.1600\n",
      "Loss: 2.2009\n",
      "Loss: 1.9714\n",
      "Loss: 2.2615\n",
      "Loss: 2.2163\n",
      "Loss: 2.1355\n",
      "Loss: 2.3396\n",
      "Loss: 2.1596\n",
      "Loss: 2.4542\n",
      "Loss: 2.0593\n",
      "Loss: 2.1271\n",
      "Loss: 2.4464\n",
      "Loss: 2.2448\n",
      "Loss: 2.1928\n",
      "Loss: 2.3177\n",
      "Loss: 2.3363\n",
      "Loss: 2.3016\n",
      "Loss: 2.1757\n",
      "Loss: 2.0013\n",
      "Loss: 2.1513\n",
      "Loss: 2.2414\n",
      "Loss: 2.1170\n",
      "Loss: 2.2532\n",
      "Loss: 2.2319\n",
      "Loss: 2.2824\n",
      "Loss: 2.1840\n",
      "Loss: 2.1343\n",
      "Loss: 2.2042\n",
      "Loss: 2.1604\n",
      "Loss: 2.0156\n",
      "Loss: 2.6257\n",
      "Loss: 2.2937\n",
      "Loss: 2.2544\n",
      "Loss: 2.0693\n",
      "Loss: 2.2766\n",
      "Loss: 2.1089\n",
      "Loss: 2.2058\n",
      "Loss: 2.1792\n",
      "Loss: 2.3348\n",
      "Loss: 2.1295\n",
      "Loss: 2.0880\n",
      "Loss: 2.2564\n",
      "Loss: 2.1359\n",
      "Loss: 2.4416\n",
      "Loss: 2.1557\n",
      "Loss: 2.2800\n",
      "Loss: 2.1117\n",
      "Loss: 2.2049\n",
      "Loss: 2.1302\n",
      "Loss: 2.1242\n",
      "Loss: 1.9874\n",
      "Loss: 2.0740\n",
      "Loss: 2.2737\n",
      "Loss: 1.9826\n",
      "Loss: 2.0818\n",
      "Loss: 2.2491\n",
      "Loss: 2.4261\n",
      "Loss: 2.1260\n",
      "Loss: 2.0600\n",
      "Loss: 2.1695\n",
      "Loss: 2.0124\n",
      "Loss: 2.4060\n",
      "Loss: 2.1590\n",
      "Loss: 2.1365\n",
      "Loss: 1.9665\n",
      "Loss: 2.0778\n",
      "Loss: 1.9946\n",
      "Loss: 2.2424\n",
      "Loss: 2.2909\n",
      "Loss: 2.1056\n",
      "Loss: 2.2034\n",
      "Loss: 2.1856\n",
      "Loss: 2.2665\n",
      "Loss: 2.1372\n",
      "Loss: 2.0291\n",
      "Loss: 2.4227\n",
      "Loss: 2.1641\n",
      "Loss: 2.0935\n",
      "Loss: 2.2059\n",
      "Loss: 1.9489\n",
      "Loss: 2.0721\n",
      "Loss: 1.9212\n",
      "Loss: 2.1396\n",
      "Loss: 2.2306\n",
      "Loss: 2.0686\n",
      "Loss: 2.2297\n",
      "Loss: 2.0451\n",
      "Loss: 2.2292\n",
      "Loss: 2.1447\n",
      "Loss: 2.2550\n",
      "Loss: 1.9869\n",
      "Loss: 2.2030\n",
      "Loss: 2.3570\n",
      "Loss: 2.1616\n",
      "Loss: 1.9384\n",
      "Loss: 2.1440\n",
      "Loss: 2.3260\n",
      "Loss: 2.2550\n",
      "Loss: 2.3907\n",
      "Loss: 2.1265\n",
      "Loss: 2.1084\n",
      "Loss: 2.0305\n",
      "Loss: 2.0548\n",
      "Loss: 2.1652\n",
      "Loss: 2.2706\n",
      "Loss: 2.1295\n",
      "Loss: 2.1558\n",
      "Loss: 2.2246\n",
      "Loss: 2.3497\n",
      "Loss: 2.0479\n",
      "Loss: 2.0266\n",
      "Loss: 2.0776\n",
      "Loss: 2.1855\n",
      "Loss: 2.2323\n",
      "Loss: 2.0514\n",
      "Loss: 1.9334\n",
      "Loss: 2.1745\n",
      "Loss: 2.1183\n",
      "Loss: 2.3597\n",
      "Loss: 2.1329\n",
      "Loss: 2.0043\n",
      "Loss: 2.3251\n",
      "Loss: 2.2375\n",
      "Loss: 1.9876\n",
      "Loss: 2.1425\n",
      "Loss: 2.2258\n",
      "Loss: 2.1966\n",
      "Loss: 2.1559\n",
      "Loss: 2.2344\n",
      "Loss: 1.9778\n",
      "Loss: 2.1685\n",
      "Loss: 2.1695\n",
      "Loss: 2.1024\n",
      "Loss: 2.0768\n",
      "Loss: 2.4743\n",
      "Loss: 2.1028\n",
      "Loss: 2.2281\n",
      "Loss: 2.3126\n",
      "Loss: 2.2265\n",
      "Loss: 2.1950\n",
      "Loss: 2.1122\n",
      "Loss: 2.1450\n",
      "Loss: 2.1886\n",
      "Loss: 2.2642\n",
      "Loss: 2.1481\n",
      "Loss: 2.1357\n",
      "Loss: 2.2584\n",
      "Loss: 2.2094\n",
      "Loss: 2.1324\n",
      "Loss: 2.3289\n",
      "Loss: 2.2023\n",
      "Loss: 2.1735\n",
      "Loss: 2.2305\n",
      "Loss: 2.3388\n",
      "Loss: 1.9508\n",
      "Loss: 2.2216\n",
      "Loss: 2.1875\n",
      "Loss: 1.9933\n",
      "Loss: 2.2060\n",
      "Loss: 2.1229\n",
      "Loss: 2.0853\n",
      "Loss: 2.2199\n",
      "Loss: 2.3692\n",
      "Loss: 2.1924\n",
      "Loss: 2.4593\n",
      "Loss: 2.0502\n",
      "Loss: 2.0511\n",
      "Loss: 2.4346\n",
      "Loss: 2.1644\n",
      "Loss: 2.1913\n",
      "Loss: 2.2224\n",
      "Loss: 2.1067\n",
      "Loss: 2.1712\n",
      "Loss: 2.1703\n",
      "Loss: 2.0210\n",
      "Loss: 2.0763\n",
      "Loss: 2.2714\n",
      "Loss: 2.1384\n",
      "Loss: 2.1520\n",
      "Loss: 2.3657\n",
      "Loss: 2.3070\n",
      "Loss: 2.0168\n",
      "Loss: 2.3101\n",
      "Loss: 2.1845\n",
      "Loss: 2.2469\n",
      "Loss: 2.2012\n",
      "Loss: 2.1638\n",
      "Loss: 2.2182\n",
      "Loss: 2.1232\n",
      "Loss: 2.0894\n",
      "Loss: 2.2308\n",
      "Loss: 2.1063\n",
      "Loss: 2.1986\n",
      "Loss: 2.1028\n",
      "Loss: 2.4212\n",
      "Loss: 2.4460\n",
      "Loss: 2.1882\n",
      "Loss: 2.3758\n",
      "Loss: 2.1936\n",
      "Loss: 2.3360\n",
      "Loss: 2.3055\n",
      "Loss: 2.1006\n",
      "Loss: 2.1763\n",
      "Loss: 2.1568\n",
      "Loss: 2.1895\n",
      "Loss: 2.1351\n",
      "Loss: 2.0701\n",
      "Loss: 2.1597\n",
      "Loss: 2.1223\n",
      "Loss: 2.0068\n",
      "Loss: 2.0004\n",
      "Loss: 2.0567\n",
      "Loss: 2.3082\n",
      "Loss: 2.0831\n",
      "Loss: 2.2284\n",
      "Loss: 2.0567\n",
      "Loss: 2.0930\n",
      "Loss: 2.1626\n",
      "Loss: 2.0336\n",
      "Loss: 2.1288\n",
      "Loss: 2.1497\n",
      "Loss: 2.1809\n",
      "Loss: 1.9312\n",
      "Loss: 2.1351\n",
      "Loss: 2.4367\n",
      "Loss: 2.1782\n",
      "Loss: 2.3230\n",
      "Loss: 2.3232\n",
      "Loss: 2.2775\n",
      "Loss: 2.5648\n",
      "Loss: 2.2608\n",
      "Loss: 2.2341\n",
      "Loss: 2.0657\n",
      "Loss: 2.2709\n",
      "Loss: 2.1703\n",
      "Loss: 1.9747\n",
      "Loss: 2.1981\n",
      "Loss: 2.0447\n",
      "Loss: 1.9239\n",
      "Loss: 2.0584\n",
      "Loss: 2.1329\n",
      "Loss: 2.2900\n",
      "Loss: 2.4406\n",
      "Loss: 2.1544\n",
      "Loss: 2.0119\n",
      "Loss: 2.1316\n",
      "Loss: 2.2121\n",
      "Loss: 2.5021\n",
      "Loss: 1.8765\n",
      "Loss: 2.2921\n",
      "Loss: 2.0952\n",
      "Loss: 2.0156\n",
      "Loss: 2.1970\n",
      "Loss: 2.0956\n",
      "Loss: 2.1053\n",
      "Loss: 2.0956\n",
      "Loss: 1.9670\n",
      "Loss: 2.2365\n",
      "Loss: 1.9392\n",
      "Loss: 2.0725\n",
      "Loss: 2.1412\n",
      "Loss: 2.2185\n",
      "Loss: 2.1872\n",
      "Loss: 2.1508\n",
      "Loss: 2.3040\n",
      "Loss: 2.0511\n",
      "Loss: 2.2901\n",
      "Loss: 2.4199\n",
      "Loss: 2.2822\n",
      "Loss: 2.1071\n",
      "Loss: 2.1857\n",
      "Loss: 2.1393\n",
      "Loss: 2.1454\n",
      "Loss: 2.4057\n",
      "Loss: 1.9221\n",
      "Loss: 2.0984\n",
      "Loss: 2.3788\n",
      "Loss: 2.2756\n",
      "Loss: 2.2740\n",
      "Loss: 2.2361\n",
      "Loss: 2.3381\n",
      "Loss: 2.1389\n",
      "Loss: 2.0185\n",
      "Loss: 2.0823\n",
      "Loss: 1.9726\n",
      "Loss: 2.1151\n",
      "Loss: 1.9919\n",
      "Loss: 2.0145\n",
      "Loss: 2.1045\n",
      "Loss: 2.1826\n",
      "Loss: 2.2832\n",
      "Loss: 2.2584\n",
      "Loss: 2.0442\n",
      "Loss: 2.0154\n",
      "Loss: 2.1325\n",
      "Loss: 2.1722\n",
      "Loss: 2.0748\n",
      "Loss: 2.2427\n",
      "Loss: 2.0037\n",
      "Loss: 2.2415\n",
      "Loss: 2.1681\n",
      "Loss: 2.3337\n",
      "Loss: 2.0370\n",
      "Loss: 2.2014\n",
      "Loss: 2.3555\n",
      "Loss: 2.2185\n",
      "Loss: 2.2307\n",
      "Loss: 2.0127\n",
      "Loss: 2.2991\n",
      "Loss: 2.2291\n",
      "Loss: 2.0312\n",
      "Loss: 2.1168\n",
      "Loss: 2.0032\n",
      "Loss: 2.2615\n",
      "Loss: 1.9147\n",
      "Loss: 2.1338\n",
      "Loss: 2.2288\n",
      "Loss: 2.1446\n",
      "Loss: 2.2254\n",
      "Loss: 2.0279\n",
      "Loss: 2.1555\n",
      "Loss: 2.0955\n",
      "Loss: 2.1142\n",
      "Loss: 2.1763\n",
      "Loss: 1.9701\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. SimCLR projection head\n",
    "class SimCLRHead(nn.Module):\n",
    "    def __init__(self, in_dim, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.net(x), dim=1)\n",
    "\n",
    "# 2. SimCLR model with Hugging Face ViT\n",
    "class SimCLRViT(nn.Module):\n",
    "    def __init__(self, vit_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(vit_name)\n",
    "        self.projector = SimCLRHead(self.vit.config.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vit(pixel_values=x)\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "        return self.projector(cls)\n",
    "\n",
    "# 3. NT-Xent contrastive loss\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    N = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2) / temperature\n",
    "    mask = torch.eye(2 * N, dtype=torch.bool).to(z.device)\n",
    "    sim.masked_fill_(mask, -9e15)\n",
    "    pos = torch.cat([torch.arange(N, 2*N), torch.arange(N)]).to(z.device)\n",
    "    numerator = torch.exp(sim[torch.arange(2 * N), pos])\n",
    "    denominator = torch.exp(sim).sum(dim=1)\n",
    "    return -torch.log(numerator / denominator).mean()\n",
    "\n",
    "# 4. SimCLR augmentations (two views per image)\n",
    "simclr_transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "class SimCLRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, _ = self.dataset[index]\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# 5. Load real dataset (e.g., CIFAR-10)\n",
    "base_dataset = datasets.CIFAR10(root='./data', download=True, train=True)\n",
    "contrastive_dataset = SimCLRDataset(base_dataset, simclr_transform)\n",
    "loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 6. Train one epoch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SimCLRViT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for batch in loader:\n",
    "    x1, x2 = batch[0].to(device), batch[1].to(device)\n",
    "    z1 = model(x1)\n",
    "    z2 = model(x2)\n",
    "    loss = nt_xent_loss(z1, z2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a98e7b-4d4f-415b-ac2a-16d56fcf45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usa il modello appreso come embedding e addestra un classificatore supervisionato\n",
    "# in esempi reali si usa DB unlabeled per la prima parte e DB labeled per il cassificatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b969a-60af-42a5-8212-d41ae9c31c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Estrai solo l'encoder (senza projector)\n",
    "encoder = model.vit  # assuming model = your trained SimCLRModel\n",
    "encoder.eval()       # non addestriamo più l'encoder\n",
    "\n",
    "# 2. Classificatore lineare\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classifier = LinearClassifier(in_dim=encoder.config.hidden_size)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3. Trasformazioni standard per classificazione\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# 4. Dataset etichettato\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_eval)\n",
    "test_ds = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_eval)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "# 5. Training loop del classificatore\n",
    "for epoch in range(5):\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        with torch.no_grad():\n",
    "            features = encoder(pixel_values=x).last_hidden_state[:, 0]  # CLS token\n",
    "\n",
    "        logits = classifier(features)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec7a53-40a0-466f-b204-5162da1d839e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
