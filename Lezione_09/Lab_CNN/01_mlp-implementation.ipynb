{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3333804c",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Implementation of Multilayer Perceptrons\n",
    " \n",
    "Multilayer perceptrons (MLPs) are not much more complex to implement than simple linear models. \n",
    "The key conceptual difference is that we now concatenate multiple layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87926c3b",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2763e5",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Let's begin again by implementing such a network from scratch.\n",
    "\n",
    "### Initializing Model Parameters\n",
    "\n",
    "Recall that Fashion-MNIST contains 10 classes,\n",
    "and that each image consists of a $28 \\times 28 = 784$\n",
    "grid of grayscale pixel values.\n",
    "As before we will disregard the spatial structure\n",
    "among the pixels for now,\n",
    "so we can think of this as a classification dataset\n",
    "with 784 input features and 10 classes.\n",
    "To begin, we will [**implement an MLP\n",
    "with one hidden layer and 256 hidden units.**]\n",
    "Both the number of layers and their width are adjustable\n",
    "(they are considered hyperparameters).\n",
    "Typically, we choose the layer widths to be divisible by larger powers of 2.\n",
    "This is computationally efficient due to the way\n",
    "memory is allocated and addressed in hardware.\n",
    "\n",
    "Again, we will represent our parameters with several tensors.\n",
    "Note that *for every layer*, we must keep track of\n",
    "one weight matrix and one bias vector.\n",
    "As always, we allocate memory\n",
    "for the gradients of the loss with respect to these parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa4fb7",
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In the code below we use `nn.Parameter`\n",
    "to automatically register\n",
    "a class attribute as a parameter to be tracked by `autograd` (:numref:`sec_autograd`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcccd30d",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n",
    "        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n",
    "        self.b2 = nn.Parameter(torch.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aeeb41",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### Model\n",
    "\n",
    "To make sure we know how everything works,\n",
    "we will [**implement the ReLU activation**] ourselves\n",
    "rather than invoking the built-in `relu` function directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af157bb",
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bd74c",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "Since we are disregarding spatial structure,\n",
    "we `reshape` each two-dimensional image into\n",
    "a flat vector of length  `num_inputs`.\n",
    "Finally, we (**implement our model**)\n",
    "with just a few lines of code. Since we use the framework built-in autograd this is all that it takes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7438b40c",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MLPScratch)\n",
    "def forward(self, X):\n",
    "    X = X.reshape((-1, self.num_inputs))\n",
    "    H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "    return torch.matmul(H, self.W2) + self.b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07056fc",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "### Training\n",
    "\n",
    "Fortunately, [**the training loop for MLPs\n",
    "is exactly the same as for softmax regression.**] We define the model, data, and trainer, then finally invoke the `fit` method on model and data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d57362",
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c068a33",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## Concise Implementation of the MLP Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5087507",
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(), nn.LazyLinear(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf391c1",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
